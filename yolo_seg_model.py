# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qrp3uvbS4cYfFSG550YrnvFpo587lsQh
"""

# --- Install Dependencies ---
# !pip uninstall transformers torch torchvision basicsr pillow -y
!pip install opencv-python-headless==4.8.1.78 numpy==1.23.5 matplotlib==3.7.1 transformers==4.36.0 torch==2.0.1 torchvision==0.15.2 scikit-learn==1.2.2 pandas==1.5.3 tabulate==0.9.0 Pillow==9.5.0 mediapipe==0.10.14 ultralytics==8.2.98 basicsr==1.4.2 --no-cache-dir --force-reinstall

!pip install ultralytics

# --- Clone and Install Real-ESRGAN ---
!rm -rf /content/Real-ESRGAN
!git clone https://github.com/xinntao/Real-ESRGAN.git /content/Real-ESRGAN
!sed -i 's/torchvision.*/torchvision==0.12.0/' /content/Real-ESRGAN/requirements.txt
!cd /content/Real-ESRGAN && pip install -r requirements.txt || echo "Requirements installation failed"
!cd /content/Real-ESRGAN && python setup.py develop --no-cache-dir || echo "Setup develop failed"
!mkdir -p /content/weights
!wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -O /content/weights/RealESRGAN_x4.pth

# --- Download YOLOv8-Face Model ---
# !wget https://github.com/Yusepp/YOLOv8-Face/releases/download/v1.0/yolov8n-face.pt -O /content/yolov8n-face.pt

# --- Add Real-ESRGAN to Python Path ---
import sys
sys.path.append('/content/Real-ESRGAN')

# face_parsing_pipeline_colab.ipynb

# --- Add Real-ESRGAN to Python Path ---
import sys
sys.path.append('/content/Real-ESRGAN')

# --- Debug Imports ---
import transformers
import torch
import torchvision
import PIL
print(f"Transformers version: {transformers.__version__}")
print(f"Transformers path: {transformers.__file__}")
print(f"Torch version: {torch.__version__}")
print(f"Torchvision version: {torchvision.__version__}")
print(f"Torchvision path: {torchvision.__file__}")
print(f"Pillow version: {PIL.__version__}")
print(f"Pillow path: {PIL.__file__}")
try:
    from transformers.models.segformer import SegformerImageProcessor
    print("SegformerImageProcessor imported successfully")
except ImportError:
    print("Failed to import SegformerImageProcessor")
try:
    from realesrgan import RealESRGANer
    print("RealESRGANer imported successfully")
except ImportError:
    print("Failed to import RealESRGANer")
try:
    import torchvision.transforms.functional as F
    print("torchvision.transforms.functional imported successfully")
except ImportError:
    print("Failed to import torchvision.transforms.functional")
try:
    import mediapipe
    print(f"MediaPipe version: {mediapipe.__version__}")
except ImportError:
    print("Failed to import MediaPipe")
try:
    from PIL import ImageFont
    print("PIL.ImageFont imported successfully")
except ImportError:
    print("Failed to import PIL.ImageFont")
try:
    from ultralytics import YOLO
    print("YOLO from ultralytics imported successfully")
except ImportError:
    print("Failed to import YOLO from ultralytics")

# --- Import Libraries ---
import cv2
import numpy as np
import matplotlib.pyplot as plt
from transformers.models.segformer import SegformerImageProcessor
from transformers import SegformerForSemanticSegmentation
from PIL import Image
import torch
import os
from sklearn.metrics import jaccard_score, f1_score
import pandas as pd
from tabulate import tabulate
import math
from ultralytics import YOLO
from realesrgan import RealESRGANer
from basicsr.archs.rrdbnet_arch import RRDBNet
import matplotlib.patches as patches

# --- Define Class Labels from config.json ---
CLASSES = {
    0: "background",
    1: "skin",  # Forehead, Cheeks
    2: "left_eyebrow",
    3: "right_eyebrow",
    4: "left_eye",  # Under_eyes
    5: "right_eye",  # Under_eyes
    6: "glasses",
    7: "left_ear",
    8: "right_ear",
    9: "earings",
    10: "nose",  # Nose
    11: "mouth",  # Lips
    12: "upper_lip",  # Lips
    13: "lower_lip",
    14: "neck",  # Neck
    15: "neck_area",
    16: "cloth",
    17: "hair",  # Hair
    18: "hat"
}

# --- Stage 1: Face Recognition and Image Validation ---
def validate_image(image_path):
    """Validate image at given path for camera facing, lighting, angle, and brightness, and compute bounding box."""
    import mediapipe as mp

    base_path = "/content/"
    os.makedirs(base_path, exist_ok=True)

    print(f"Checking image: {image_path}")
    if not os.path.exists(image_path):
        print(f"Error: Image path does not exist: {image_path}")
        return False, f"Image path does not exist: {image_path}", None, None

    # Validate file extension
    if not image_path.lower().endswith(('.jpg', '.png', '.webp')):
        print(f"Error: Unsupported file format: {image_path}")
        return False, "Unsupported file format. Use JPG, PNG, or WebP.", None, None

    # Convert WebP to PNG if needed
    if image_path.lower().endswith('.webp'):
        print("Converting WebP to PNG...")
        with Image.open(image_path) as img:
            png_path = os.path.join(base_path, os.path.basename(image_path).replace('.webp', '.png'))
            img.save(png_path, "PNG")
        image_path = png_path

    # Read image
    print("Reading image...")
    image = cv2.imread(image_path)
    if image is None:
        print(f"Error: Cannot read image at {image_path}")
        return False, f"Cannot read image at {image_path}", None, None
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    print("Image read successfully, shape:", image_rgb.shape)

    # Face detection and pose estimation with MediaPipe
    print("Detecting face with MediaPipe...")
    mp_face_mesh = mp.solutions.face_mesh
    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:
        results = face_mesh.process(image_rgb)
        if not results.multi_face_landmarks:
            print("Error: No face detected")
            return False, "No face detected", None, None

        # Estimate pose and bounding box using landmarks
        face_landmarks = results.multi_face_landmarks[0]
        h, w, _ = image_rgb.shape
        landmarks = [(lm.x * w, lm.y * h, lm.z * w) for lm in face_landmarks.landmark]

        # Key landmarks for pose estimation
        nose_tip = landmarks[1]  # Nose tip (index 1)
        left_eye = landmarks[33]  # Left eye corner (index 33)
        right_eye = landmarks[263]  # Right eye corner (index 263)
        chin = landmarks[152]  # Chin (index 152)

        # Print key landmark coordinates
        print("\nMediaPipe Landmark Coordinates:")
        print(f"Nose Tip: {nose_tip}")
        print(f"Left Eye: {left_eye}")
        print(f"Right Eye: {right_eye}")
        print(f"Chin: {chin}")

        # Compute bounding box
        x_coords, y_coords = zip(*[(lm.x * w, lm.y * h) for lm in face_landmarks.landmark])
        x_min, x_max = min(x_coords), max(x_coords)
        y_min, y_max = min(y_coords), max(y_coords)
        padding = 10
        bbox = (max(0, int(x_min - padding)), max(0, int(y_min - padding)),
                min(w, int(x_max + padding)), min(h, int(y_max + padding)))

        # Compute yaw (left-right rotation)
        dx = right_eye[0] - left_eye[0]
        dy = right_eye[1] - left_eye[1]
        yaw = math.degrees(math.atan2(dy, dx))

        # Compute pitch (up-down rotation)
        dz = nose_tip[2] - chin[2]
        pitch = math.degrees(math.atan2(dz, h / 2))

        # Compute roll (tilt) using eye landmarks' vertical alignment
        roll = math.degrees(math.atan2(dy, dx))

        # Print raw pose angles
        print("\nMediaPipe Pose Angles (degrees):")
        print(f"Yaw: {yaw:.1f}")
        print(f"Pitch: {pitch:.1f}")
        print(f"Roll: {roll:.1f}")

        if abs(yaw) > 15 or abs(pitch) > 25 or abs(roll) > 90:
            print("Error: Invalid face angle detected")
            return False, f"Invalid face angle (yaw: {yaw:.1f}, pitch: {pitch:.1f}, roll: {roll:.1f})", None, None

    # Lighting and brightness check
    print("Checking lighting and brightness...")
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    mean_brightness = np.mean(gray)
    if mean_brightness < 50:
        print(f"Error: Image too dark (mean brightness: {mean_brightness:.1f})")
        return False, f"Image too dark (mean brightness: {mean_brightness:.1f})", None, None

    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
    hist_norm = hist.ravel() / hist.sum()
    entropy = -np.sum(hist_norm * np.log2(hist_norm + 1e-7))
    if entropy < 4:
        print(f"Error: Low contrast (entropy: {entropy:.1f})")
        return False, f"Low contrast (entropy: {entropy:.1f})", None, None

    return True, "Image valid", image_rgb, bbox

# --- Stage 2: Image Enhancement with YOLO and ESRGAN ---
def enhance_image(image_rgb):
    """Enhance image using YOLOv8-Face for cropping and ESRGAN for super-resolution."""
    # Convert NumPy array to PIL image for YOLO
    from PIL import Image as PILImage
    image_pil = PILImage.fromarray(image_rgb)

    # YOLOv8-Face for face detection and cropping
    try:
        yolo_model = YOLO('/content/yolov8n_100e.pt')
        results = yolo_model.predict(image_pil, conf=0.5)

        if not results[0].boxes:
            raise ValueError("No face detected by YOLOv8-Face")

        # Crop face with padding
        x1, y1, x2, y2 = results[0].boxes.xyxy[0].cpu().numpy().astype(int)
        padding = 10
        face_crop = image_rgb[max(0, y1-padding):min(image_rgb.shape[0], y2+padding),
                             max(0, x1-padding):min(image_rgb.shape[1], x2+padding)]
        face_crop = cv2.resize(face_crop, (256, 256))  # ESRGAN input size
    except Exception as e:
        print(f"YOLOv8-Face failed: {str(e)}. Using full image...")
        face_crop = cv2.resize(image_rgb, (256, 256))

    # ESRGAN for super-resolution
    try:
        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)
        esrgan = RealESRGANer(
            scale=4,
            model_path='/content/weights/RealESRGAN_x4.pth',
            model=model,
            device='cuda' if torch.cuda.is_available() else 'cpu',
            tile=0,
            tile_pad=10,
            pre_pad=0
        )
        face_crop_bgr = cv2.cvtColor(face_crop, cv2.COLOR_RGB2BGR)
        enhanced_image, _ = esrgan.enhance(face_crop_bgr, outscale=4)
        enhanced_image = cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2RGB)
        enhanced_image = cv2.resize(enhanced_image, (512, 512))  # Output size
    except Exception as e:
        print(f"ESRGAN failed: {str(e)}. Using resized crop...")
        enhanced_image = cv2.resize(face_crop, (512, 512))

    print(f"Enhanced image shape: {enhanced_image.shape}")
    cv2.imwrite("/content/enhanced_image.jpg", cv2.cvtColor(enhanced_image, cv2.COLOR_RGB2BGR))
    return enhanced_image

# --- Stage 3: Face Parsing, Segmentation, and RAG Validation ---
def parse_face(image):
    """Segment face regions using jonathandinu/face-parsing and validate with RAG."""
    # Preprocessing with CLAHE
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    l_clahe = clahe.apply(l)
    lab_merge = cv2.merge((l_clahe, a, b))
    image_processed = cv2.cvtColor(lab_merge, cv2.COLOR_LAB2RGB)

    device = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
    try:
        processor = SegformerImageProcessor.from_pretrained("jonathandinu/face-parsing")
    except Exception as e:
        print(f"SegformerImageProcessor failed: {str(e)}. Trying AutoImageProcessor...")
        from transformers import AutoImageProcessor
        processor = AutoImageProcessor.from_pretrained("jonathandinu/face-parsing")

    model = SegformerForSemanticSegmentation.from_pretrained("jonathandinu/face-parsing")
    model.to(device).eval()

    img = Image.fromarray(image_processed)
    inputs = processor(images=img, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        print(f"Logits shape: {logits.shape}")
        upsampled_logits = torch.nn.functional.interpolate(
            logits, size=(512, 512), mode="bilinear", align_corners=False
        )
        pred_seg = upsampled_logits.argmax(dim=1)[0].cpu().numpy()
        print(f"Predicted segmentation shape: {pred_seg.shape}")

    # Print pixel counts for all classes
    print("\nPixel counts per class:")
    for label_id, label_name in CLASSES.items():
        pixel_count = np.sum(pred_seg == label_id)
        print(f"{label_name} ({label_id}): {pixel_count} pixels")

    # Generate masks for all classes
    all_masks = {label_name: (pred_seg == label_id).astype(np.uint8) * 255 for label_id, label_name in CLASSES.items()}

    # Map labels to requested regions
    masks = {
        "forehead": (pred_seg == 1).astype(np.uint8) * 255,  # skin
        "cheeks": (pred_seg == 1).astype(np.uint8) * 255,  # skin
        "nose": (pred_seg == 10).astype(np.uint8) * 255,  # nose
        "under_eyes": ((pred_seg == 4) | (pred_seg == 5)).astype(np.uint8) * 255,  # left/right eye
        "lips": ((pred_seg == 11) | (pred_seg == 12)).astype(np.uint8) * 255,  # mouth or upper_lip
        "neck": (pred_seg == 14).astype(np.uint8) * 255,  # neck
        "hair": (pred_seg == 17).astype(np.uint8) * 255  # hair
    }

    # Save all masks
    output_dir = "/content/masks/"
    os.makedirs(output_dir, exist_ok=True)
    for label_name, mask in all_masks.items():
        cv2.imwrite(f"{output_dir}{label_name}_mask.jpg", mask)

    # RAG Validation (Placeholder)
    rag_results = rag_validate(masks)

    return masks, all_masks, pred_seg, rag_results

def rag_validate(masks):
    """Placeholder for RAG-based validation against dermatology knowledge base."""
    results = {}
    for region, mask in masks.items():
        coverage = np.sum(mask > 0) / (512 * 512)
        expected_coverage = {
            "forehead": (0.1, 0.2),
            "cheeks": (0.2, 0.3),
            "nose": (0.05, 0.1),
            "under_eyes": (0.05, 0.1),
            "lips": (0.02, 0.05),
            "neck": (0.1, 0.2),
            "hair": (0.1, 0.3)
        }
        cov_min, cov_max = expected_coverage[region]
        is_valid = cov_min <= coverage <= cov_max
        results[region] = {"coverage": coverage, "valid": is_valid}

    return results

# --- Compute Metrics ---
def compute_metrics(masks, ground_truth, pred_seg):
    """Compute IoU, Dice, Pixel Accuracy, and heuristic metrics."""
    metrics = []

    if ground_truth is not None:
        gt_labels = {
            "forehead": (ground_truth == 1).astype(np.uint8),
            "cheeks": (ground_truth == 1).astype(np.uint8),
            "nose": (ground_truth == 10).astype(np.uint8),
            "under_eyes": ((ground_truth == 4) | (ground_truth == 5)).astype(np.uint8),
            "lips": ((ground_truth == 11) | (ground_truth == 12)).astype(np.uint8),
            "neck": (ground_truth == 14).astype(np.uint8),
            "hair": (ground_truth == 17).astype(np.uint8)
        }

        for region in masks.keys():
            pred = masks[region] // 255
            gt = gt_labels[region]
            iou = jaccard_score(gt.flatten(), pred.flatten(), average='binary')
            dice = f1_score(gt.flatten(), pred.flatten(), average='binary')
            accuracy = np.mean(gt == pred)
            metrics.append([region, iou, dice, accuracy, "N/A"])
    else:
        total_pixels = 512 * 512
        for region, mask in masks.items():
            coverage = np.sum(mask > 0) / total_pixels
            overlap_score = 0
            for other_region, other_mask in masks.items():
                if other_region != region:
                    overlap = np.sum((mask > 0) & (other_mask > 0)) / np.sum(mask > 0) if np.sum(mask > 0) > 0 else 0
                    overlap_score += overlap
            overlap_score = 1 - (overlap_score / (len(masks) - 1)) if len(masks) > 1 else 1
            expected_coverage = {
                "forehead": (0.1, 0.2),
                "cheeks": (0.2, 0.3),
                "nose": (0.05, 0.1),
                "under_eyes": (0.05, 0.1),
                "lips": (0.02, 0.05),
                "neck": (0.1, 0.2),
                "hair": (0.1, 0.3)
            }
            cov_min, cov_max = expected_coverage[region]
            coverage_validity = 1 if cov_min <= coverage <= cov_max else 0.5
            heuristic_score = (coverage_validity + overlap_score) / 2
            metrics.append([region, "N/A", "N/A", "N/A", heuristic_score])

    headers = ["Region", "IoU", "Dice", "Pixel Accuracy", "Heuristic Score"]
    df = pd.DataFrame(metrics, columns=headers)
    return df

# --- Visualize Results ---
def visualize_results(image_rgb, enhanced_image, masks, all_masks, rag_results, metrics_df, bbox):
    """Visualize original image with bounding box, enhanced image, masks, and metrics."""
    import matplotlib.patches as patches

    plt.figure(figsize=(15, 15))

    # Subplot 1: Original image with green bounding box
    plt.subplot(3, 4, 1)
    plt.title("Original Image with Bounding Box")
    plt.imshow(image_rgb)
    if bbox:
        x_min, y_min, x_max, y_max = bbox
        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='green', facecolor='none')
        plt.gca().add_patch(rect)
    plt.axis('off')

    # Subplot 2: Enhanced image
    plt.subplot(3, 4, 2)
    plt.title("Enhanced Image")
    plt.imshow(enhanced_image)
    plt.axis('off')

    # Subplots 3-9: Masks
    for i, (region, mask) in enumerate(masks.items(), 3):
        plt.subplot(3, 4, i)
        plt.title(f"{region} (Valid: {rag_results[region]['valid']})")
        plt.imshow(mask, cmap='gray')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

    print("\nMetrics Table:")
    print(tabulate(metrics_df, headers='keys', tablefmt='psql', floatfmt=".3f"))

    print("\nRAG Validation Results:")
    for region, result in rag_results.items():
        print(f"{region}: Coverage={result['coverage']:.3f}, Valid={result['valid']}")

# --- Print Class Information ---
def print_class_info():
    """Print available classes from config.json."""
    print("\nAvailable Classes (from config.json):")
    for label_id, label_name in CLASSES.items():
        print(f"Label {label_id}: {label_name}")

# --- Main Execution ---
if __name__ == "__main__":
    # Step 1: Print class info
    print_class_info()

    # Stage 1: Validate image
    default_image_path = "/content/acneface.jpg"  # Update if different filename
    is_valid, message, image_rgb, bbox = validate_image(default_image_path)
    if not is_valid:
        raise ValueError(message)
    print(message)

    # Load ground truth mask (optional)
    ground_truth_path = "/content/ground_truth.png"  # Update if different filename
    ground_truth = None
    if os.path.exists(ground_truth_path):
        ground_truth = cv2.imread(ground_truth_path, cv2.IMREAD_GRAYSCALE)
        if ground_truth is not None:
            ground_truth = cv2.resize(ground_truth, (512, 512))
            print(f"Ground truth mask shape: {ground_truth.shape}")
        else:
            print(f"Failed to load ground truth mask at {ground_truth_path}")

    # Stage 2: Enhance image
    enhanced_image = enhance_image(image_rgb)

    # Stage 3: Face parsing and RAG
    masks, all_masks, pred_seg, rag_results = parse_face(enhanced_image)

    # Compute metrics
    metrics_df = compute_metrics(masks, ground_truth, pred_seg)

    # Visualize results
    visualize_results(image_rgb, enhanced_image, masks, all_masks, rag_results, metrics_df, bbox)

